{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gblgWMQXxH8"
      },
      "source": [
        "# Reconnaissance Automatique de la Parole - Jour 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmNHF6KG7qMl",
        "outputId": "638dabc4-76af-47e8-a225-5202fc2b1fc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Levenshtein in /usr/local/lib/python3.12/dist-packages (0.27.3)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from Levenshtein) (3.14.3)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install Levenshtein optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S1edRcGnqlw8"
      },
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "import torch\n",
        "from torchaudio.datasets import LIBRISPEECH\n",
        "import torch.nn.functional as F\n",
        "from torchaudio import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import Levenshtein\n",
        "import math\n",
        "import optuna\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zhQp1RJEN2mX"
      },
      "outputs": [],
      "source": [
        "# Model selection parameter - change this to 'MLP', 'CNN', 'GRU', or 'Transformer'\n",
        "MODEL_TYPE = 'MLP'  # Options: 'MLP', 'CNN', 'GRU', 'Transformer'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFSDfq7Uqvsk",
        "outputId": "865ba8ef-50a9-4a44-bf7e-17698fcc764d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data\n",
            "Data Downloaded !\n",
            "Dataset size: 2703 samples\n",
            "First sample shape: torch.Size([1, 93680])\n"
          ]
        }
      ],
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to handle variable-length audio\"\"\"\n",
        "    waveforms = [item[0] for item in batch]\n",
        "    sample_rates = [item[1] for item in batch]\n",
        "    transcripts = [item[2] for item in batch]\n",
        "    speaker_ids = [item[3] for item in batch]\n",
        "    chapter_ids = [item[4] for item in batch]\n",
        "    utterance_ids = [item[5] for item in batch]\n",
        "\n",
        "    return waveforms, sample_rates, transcripts, speaker_ids, chapter_ids, utterance_ids\n",
        "\n",
        "def manage_data(batch_size, num_workers):\n",
        "    train_dataset = LIBRISPEECH(\"./data\", url=\"dev-clean\", download=True)\n",
        "    print(\"Data Downloaded !\")\n",
        "\n",
        "    print(f\"Dataset size: {len(train_dataset)} samples\")\n",
        "    print(f\"First sample shape: {train_dataset[0][0].shape}\")\n",
        "\n",
        "    # Create DataLoader for batch processing\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "    return train_loader\n",
        "\n",
        "def load_data():\n",
        "    print(\"Downloading data\")\n",
        "\n",
        "    os.makedirs(\"data\", exist_ok=True)\n",
        "    # Use 'dev-clean' for testing, 'train-clean-100' for training\n",
        "    train_loader = manage_data(batch_size=8, num_workers=2)\n",
        "\n",
        "load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5rXVNS-9oXZ",
        "outputId": "4b6826fc-3350-4566-bef4-824f89432e67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CUDA GPU\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# DEVICE MANAGEMENT\n",
        "# -----------------------------\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"Using CUDA GPU\")\n",
        "        return torch.device(\"cuda\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        print(\"Using Apple MPS\")\n",
        "        return torch.device(\"mps\")\n",
        "    else:\n",
        "        print(\"Using CPU\")\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "device = get_device()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vTkVfvKJEB3"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "304W6VXs8jVs"
      },
      "outputs": [],
      "source": [
        "def _MFCC(waveform, sample_rate):\n",
        "    transform = transforms.MFCC(\n",
        "        sample_rate=sample_rate,\n",
        "        n_mfcc=13,\n",
        "        melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 23, \"center\": False}\n",
        "    )\n",
        "    mfcc = transform(waveform)\n",
        "    return mfcc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "k68rzOJKq-61"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size=13, hidden_size=256, output_size=29, num_layers=3):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(input_size, hidden_size))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Dropout(0.2))\n",
        "\n",
        "        for _ in range(num_layers - 2):\n",
        "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.2))\n",
        "\n",
        "        layers.append(nn.Linear(hidden_size, output_size))\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "        self.loss_fn = nn.CTCLoss(blank=28, zero_infinity=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "    def loss(self, log_probs, targets, input_lengths, target_lengths):\n",
        "        return self.loss_fn(log_probs, targets, input_lengths, target_lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bC4HQN0-JOr6"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, n_classes=30, n_mels=40):  # 30 phonemes or characters\n",
        "        super().__init__()\n",
        "\n",
        "        # 1D convolutions along time axis\n",
        "        # Input channels = n_mels (frequency bins treated as channels)\n",
        "        self.conv1 = nn.Conv1d(n_mels, 128, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool1d(2)  # pool along time axis\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # final classifier\n",
        "        self.classifier = nn.Linear(512, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, 1, F, T] from spectrogram (mel_spec gives [1, n_mels, time])\n",
        "        # Reshape to [B, F, T] for 1D conv along time\n",
        "        if x.dim() == 4:\n",
        "            # Input is [B, 1, F, T]\n",
        "            x = x.squeeze(1)  # -> [B, F, T]\n",
        "        elif x.dim() == 3:\n",
        "            # Input is already [B, F, T] or needs transpose\n",
        "            # Check if last dim is n_mels (needs transpose)\n",
        "            if x.shape[-1] == self.conv1.in_channels:\n",
        "                x = x.transpose(1, 2)  # [B, T, F] -> [B, F, T]\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)  # -> [B, 128, T/2]\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)  # -> [B, 256, T/4]\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x)  # -> [B, 512, T/8]\n",
        "\n",
        "        # Transpose to [B, T', C] for time-distributed classification\n",
        "        x = x.transpose(1, 2)  # -> [B, T', 512]\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        out = self.classifier(x)  # [B, T', n_classes]\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8H0KjInXPOGc"
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_dim=40, hidden_dim=128, num_layers=2, n_classes=30):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_dim * 2, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, F]\n",
        "        out, _ = self.gru(x)       # out: [B, T, hidden*2]\n",
        "        logits = self.classifier(out)\n",
        "        return logits              # [B, T, n_classes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MfPpkst7XyT4"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=10000):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, d_model]\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, input_dim=40, d_model=256, nhead=8, num_layers=6, dim_feedforward=1024, n_classes=29, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Transformer-based speech recognition model\n",
        "\n",
        "        Args:\n",
        "            input_dim: Input feature dimension (e.g., 40 for mel spectrograms)\n",
        "            d_model: Dimension of the model\n",
        "            nhead: Number of attention heads\n",
        "            num_layers: Number of transformer encoder layers\n",
        "            dim_feedforward: Dimension of feedforward network\n",
        "            n_classes: Number of output classes\n",
        "            dropout: Dropout rate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Input projection\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Output classifier\n",
        "        self.classifier = nn.Linear(d_model, n_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, F] where F is input_dim (e.g., 40 mel bins)\n",
        "\n",
        "        # Project to d_model\n",
        "        x = self.input_projection(x)  # [B, T, d_model]\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Transformer encoding\n",
        "        x = self.transformer_encoder(x)  # [B, T, d_model]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(x)  # [B, T, n_classes]\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf2ew4X6JFYn"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hzFZGDOv8wnG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_model(batch_size, num_workers, num_epochs=10, save_path='model_checkpoint.pth', model_type='Transformer'):\n",
        "    # -------------------------------\n",
        "    # 1. GPU / MPS / CPU management\n",
        "    # -------------------------------\n",
        "    device = torch.device(\n",
        "        \"cuda\" if torch.cuda.is_available()\n",
        "        else \"mps\" if torch.backends.mps.is_available()\n",
        "        else \"cpu\"\n",
        "    )\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    train_loader = manage_data(batch_size, num_workers)\n",
        "\n",
        "    # -------------------------------\n",
        "    # 2. Initialize Transforms ONCE and move to Device\n",
        "    # -------------------------------\n",
        "    # We define these outside the loop to avoid re-creating them\n",
        "    # and to ensure the internal window buffers are on the GPU.\n",
        "\n",
        "    # For CNN / GRU / Transformer\n",
        "    melspec_transform = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=16000, # Assuming 16k, adjust if your data differs\n",
        "        n_mels=40,\n",
        "        n_fft=400,\n",
        "        hop_length=80\n",
        "    ).to(device)\n",
        "\n",
        "    # For MLP (Replacing your _MFCC function with the official class to be safe)\n",
        "    mfcc_transform = torchaudio.transforms.MFCC(\n",
        "        sample_rate=16000,\n",
        "        n_mfcc=13,\n",
        "        melkwargs={\"n_fft\": 400, \"hop_length\": 80, \"n_mels\": 40}\n",
        "    ).to(device)\n",
        "\n",
        "    # -------------------------------\n",
        "    # 3. Initialize Model\n",
        "    # -------------------------------\n",
        "    if model_type == 'MLP':\n",
        "        model = MLP(input_size=13, hidden_size=128, output_size=29)\n",
        "        print(\"Using MLP\")\n",
        "    elif model_type == 'CNN':\n",
        "        model = CNN(n_classes=29, n_mels=40)\n",
        "        print(\"Using CNN\")\n",
        "    elif model_type == 'GRU':\n",
        "        model = GRU(input_dim=40, hidden_dim=128, num_layers=2, n_classes=29)\n",
        "        print(\"Using GRU\")\n",
        "    elif model_type == 'Transformer':\n",
        "        model = Transformer(input_dim=40, d_model=256, nhead=8, num_layers=6, n_classes=29)\n",
        "        print(\"Using Transformer\")\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model_type\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.train(True)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "    ctc_loss = torch.nn.CTCLoss(blank=28, zero_infinity=True)\n",
        "    char2idx = {c: i for i, c in enumerate(\"abcdefghijklmnopqrstuvwxyz '\")}\n",
        "\n",
        "    # -------------------------------\n",
        "    # 4. Training Loop\n",
        "    # -------------------------------\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, (waveforms, sample_rates, transcripts, _, _, _) in enumerate(train_loader):\n",
        "\n",
        "            batch_log_probs = []\n",
        "            batch_targets = []\n",
        "            batch_input_lengths = []\n",
        "            batch_target_lengths = []\n",
        "\n",
        "            for i, (waveform, sample_rate) in enumerate(zip(waveforms, sample_rates)):\n",
        "\n",
        "                # Move waveform to device\n",
        "                waveform = waveform.to(device)\n",
        "\n",
        "                # -------------------------------\n",
        "                # Feature extraction (Using pre-loaded transforms)\n",
        "                # -------------------------------\n",
        "                if model_type == 'MLP':\n",
        "                    # Use the pre-initialized MFCC transform\n",
        "                    features = mfcc_transform(waveform)\n",
        "                    features = features.squeeze(0).transpose(0, 1)\n",
        "\n",
        "                elif model_type in ['CNN', 'GRU', 'Transformer']:\n",
        "                    # Use the pre-initialized MelSpec transform\n",
        "                    features = melspec_transform(waveform)\n",
        "                    features = features.clamp(min=1e-9).log2()\n",
        "\n",
        "                    if model_type == 'CNN':\n",
        "                        features = features.unsqueeze(0)\n",
        "                    elif model_type in ['GRU', 'Transformer']:\n",
        "                        # Expects [B, T, F] -> transpose\n",
        "                        features = features.squeeze(0).transpose(0, 1).unsqueeze(0)\n",
        "\n",
        "                # Target processing\n",
        "                target = torch.tensor(\n",
        "                    [char2idx[c] for c in transcripts[i].lower() if c in char2idx],\n",
        "                    dtype=torch.long,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                # Calculate input length for CTC\n",
        "                if model_type == 'MLP':\n",
        "                    input_len = features.size(0)\n",
        "                elif model_type == 'CNN':\n",
        "                    input_len = features.size(-1) // 8 # Adjust based on your CNN strides\n",
        "                elif model_type in ['GRU', 'Transformer']:\n",
        "                    input_len = features.size(1)\n",
        "\n",
        "                # Skip if target is longer than input (CTC requirement)\n",
        "                if input_len <= len(target):\n",
        "                    continue\n",
        "\n",
        "                # Forward Pass\n",
        "                logits = model(features)\n",
        "\n",
        "                # Squeeze batch dim if necessary (logic from your original code)\n",
        "                if model_type in ['CNN', 'GRU', 'Transformer']:\n",
        "                    logits = logits.squeeze(0)\n",
        "\n",
        "                log_probs = F.log_softmax(logits, dim=1)\n",
        "\n",
        "                batch_log_probs.append(log_probs)\n",
        "                batch_targets.append(target)\n",
        "                batch_input_lengths.append(log_probs.size(0))\n",
        "                batch_target_lengths.append(len(target))\n",
        "\n",
        "            if len(batch_log_probs) == 0:\n",
        "                continue\n",
        "\n",
        "            # -------------------------------\n",
        "            # Pad sequences and Compute Loss\n",
        "            # -------------------------------\n",
        "            max_input_len = max(batch_input_lengths)\n",
        "            padded_log_probs = torch.zeros(\n",
        "                max_input_len, len(batch_log_probs), 29,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            for i, log_probs in enumerate(batch_log_probs):\n",
        "                padded_log_probs[:log_probs.size(0), i, :] = log_probs\n",
        "\n",
        "            concatenated_targets = torch.cat(batch_targets)\n",
        "            input_lengths = torch.tensor(batch_input_lengths, dtype=torch.long, device=device)\n",
        "            target_lengths = torch.tensor(batch_target_lengths, dtype=torch.long, device=device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = ctc_loss(padded_log_probs, concatenated_targets, input_lengths, target_lengths)\n",
        "\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"NaN loss detected at batch {batch_idx}\")\n",
        "                continue\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            if (batch_idx + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_idx+1}, Avg Loss: {total_loss/num_batches:.4f}\")\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1} completed. Avg loss = {total_loss/num_batches:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Model saved to {save_path}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS0mNQTwJG3G"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM3pf-Is8oWw"
      },
      "outputs": [],
      "source": [
        "def decode_predictions(log_probs, idx2char, blank_idx=28):\n",
        "    predictions = torch.argmax(log_probs, dim=1)\n",
        "\n",
        "    decoded = []\n",
        "    prev_char = None\n",
        "    for pred in predictions:\n",
        "        pred_idx = pred.item()\n",
        "        if pred_idx != blank_idx and pred_idx != prev_char:\n",
        "            decoded.append(idx2char[pred_idx])\n",
        "        prev_char = pred_idx\n",
        "    return ''.join(decoded)\n",
        "\n",
        "def calculate_wer(reference, hypothesis):\n",
        "    ref_words = reference.split()\n",
        "    hyp_words = hypothesis.split()\n",
        "    if len(ref_words) == 0:\n",
        "        return 0.0 if len(hyp_words) == 0 else 1.0\n",
        "    distance = Levenshtein.distance(' '.join(ref_words), ' '.join(hyp_words))\n",
        "    return distance / len(ref_words)\n",
        "\n",
        "def calculate_cer(reference, hypothesis):\n",
        "    if len(reference) == 0:\n",
        "        return 0.0 if len(hypothesis) == 0 else 1.0\n",
        "    distance = Levenshtein.distance(reference, hypothesis)\n",
        "    return distance / len(reference)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#   E V A L U A T I O N    W I T H    G P U / M P S / C P U\n",
        "# ============================================================\n",
        "\n",
        "def evaluate_model(model, dataloader, model_type='MLP', device=None, num_samples=None):\n",
        "    # -----------------------------\n",
        "    # Device selection\n",
        "    # -----------------------------\n",
        "    if device is None:\n",
        "        device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available()\n",
        "            else \"mps\" if torch.backends.mps.is_available()\n",
        "            else \"cpu\"\n",
        "        )\n",
        "    print(f\"\\nEvaluating on device: {device}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    chars = \"abcdefghijklmnopqrstuvwxyz '\"\n",
        "    char2idx = {c: i for i, c in enumerate(chars)}\n",
        "    idx2char = {i: c for c, i in char2idx.items()}\n",
        "\n",
        "    # Check sample rate - LibriSpeech is 16000Hz\n",
        "    sample_rate = 16000\n",
        "\n",
        "    # Transform for MLP\n",
        "    mfcc_transform = torchaudio.transforms.MFCC(\n",
        "        sample_rate=sample_rate,\n",
        "        n_mfcc=13,\n",
        "        melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 23, \"center\": False}\n",
        "    ).to(device)\n",
        "\n",
        "    # Transform for CNN / GRU / Transformer\n",
        "    melspec_transform = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=sample_rate,\n",
        "        n_mels=40,\n",
        "        n_fft=400,\n",
        "        hop_length=160\n",
        "    ).to(device)\n",
        "\n",
        "    total_wer = 0.0\n",
        "    total_cer = 0.0\n",
        "    num_processed = 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (waveforms, sample_rates, transcripts, _, _, _) in enumerate(dataloader):\n",
        "\n",
        "            for i, (waveform, sr, transcript) in enumerate(zip(waveforms, sample_rates, transcripts)):\n",
        "\n",
        "                # Move waveform to device\n",
        "                waveform = waveform.to(device)\n",
        "\n",
        "                # ---------------------------------------------------\n",
        "                # Feature extraction (Using pre-loaded transforms)\n",
        "                # ---------------------------------------------------\n",
        "                if model_type == 'MLP':\n",
        "                    # Use the pre-initialized MFCC transform\n",
        "                    features = mfcc_transform(waveform)\n",
        "                    features = features.squeeze(0).transpose(0, 1)\n",
        "\n",
        "                elif model_type in ['CNN', 'GRU', 'Transformer']:\n",
        "                    # Use the pre-initialized MelSpec transform\n",
        "                    features = melspec_transform(waveform)\n",
        "                    features = features.clamp(min=1e-9).log2()\n",
        "\n",
        "                    if model_type == 'CNN':\n",
        "                        features = features.unsqueeze(0)\n",
        "                    elif model_type in ['GRU', 'Transformer']:\n",
        "                        # Expects [B, T, F] -> transpose\n",
        "                        features = features.squeeze(0).transpose(0, 1).unsqueeze(0)\n",
        "\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown model_type: {model_type}\")\n",
        "\n",
        "                # Forward → logits\n",
        "                logits = model(features)\n",
        "\n",
        "                # Remove batch dim if needed\n",
        "                if model_type in ['CNN', 'GRU', 'Transformer']:\n",
        "                    logits = logits.squeeze(0)\n",
        "\n",
        "                log_probs = F.log_softmax(logits, dim=1)\n",
        "\n",
        "                # Decode text\n",
        "                predicted_text = decode_predictions(log_probs, idx2char)\n",
        "                reference_text = transcript.lower()\n",
        "\n",
        "                # Metrics\n",
        "                wer = calculate_wer(reference_text, predicted_text)\n",
        "                cer = calculate_cer(reference_text, predicted_text)\n",
        "\n",
        "                total_wer += wer\n",
        "                total_cer += cer\n",
        "                num_processed += 1\n",
        "\n",
        "                # Print first few samples\n",
        "                if num_processed <= 5:\n",
        "                    print(f\"\\nSample {num_processed}:\")\n",
        "                    print(f\"  Reference:  {reference_text}\")\n",
        "                    print(f\"  Predicted:  {predicted_text}\")\n",
        "                    print(f\"  WER: {wer:.2%}, CER: {cer:.2%}\")\n",
        "\n",
        "                if num_samples and num_processed >= num_samples:\n",
        "                    break\n",
        "\n",
        "            if num_samples and num_processed >= num_samples:\n",
        "                break\n",
        "\n",
        "    avg_wer = total_wer / num_processed if num_processed > 0 else 0\n",
        "    avg_cer = total_cer / num_processed if num_processed > 0 else 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"AVERAGE METRICS (over {num_processed} samples)\")\n",
        "    print(f\"  Average WER: {avg_wer:.2%}\")\n",
        "    print(f\"  Average CER: {avg_cer:.2%}\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    return avg_wer, avg_cer\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#     L O A D   &   E V A L U A T E\n",
        "# ============================================================\n",
        "\n",
        "def load_and_evaluate(model_path=None, model_type='MLP', batch_size=8, num_workers=2, num_samples=50):\n",
        "\n",
        "    # -----------------------------\n",
        "    # Device selection\n",
        "    # -----------------------------\n",
        "    device = torch.device(\n",
        "        \"cuda\" if torch.cuda.is_available()\n",
        "        else \"mps\" if torch.backends.mps.is_available()\n",
        "        else \"cpu\"\n",
        "    )\n",
        "    print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "    # Dataset\n",
        "    eval_dataset = LIBRISPEECH(\"./data\", url=\"test-clean\", download=True)\n",
        "    eval_loader = DataLoader(\n",
        "        eval_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # Model init\n",
        "    print(f\"Initializing {model_type} model...\")\n",
        "    if model_type == 'MLP':\n",
        "        model = MLP(input_size=13, hidden_size=128, output_size=29)\n",
        "    elif model_type == 'CNN':\n",
        "        model = CNN(n_classes=29, n_mels=40)\n",
        "    elif model_type == 'GRU':\n",
        "        model = GRU(input_dim=40, hidden_dim=128, num_layers=2, n_classes=29)\n",
        "    elif model_type == 'Transformer':\n",
        "        model = Transformer(input_dim=40, d_model=256, nhead=8, num_layers=6, n_classes=29)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "    # Load weights\n",
        "    if model_path:\n",
        "        print(f\"Loading model from {model_path}\")\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    else:\n",
        "        print(\"Warning: No model_path provided → evaluating random weights\")\n",
        "\n",
        "    # Evaluate\n",
        "    return evaluate_model(\n",
        "        model,\n",
        "        eval_loader,\n",
        "        model_type=model_type,\n",
        "        device=device,\n",
        "        num_samples=num_samples\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmUJreLe83am",
        "outputId": "99a3430e-b93b-4d1c-a253-a06f6d210db4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Data Downloaded !\n",
            "Dataset size: 2703 samples\n",
            "First sample shape: torch.Size([1, 93680])\n",
            "Using MLP\n",
            "Epoch 1, Batch 10, Avg Loss: 38.0441\n",
            "Epoch 1, Batch 20, Avg Loss: 31.3048\n",
            "Epoch 1, Batch 30, Avg Loss: 28.6114\n",
            "Epoch 1, Batch 40, Avg Loss: 26.7469\n",
            "Epoch 1, Batch 50, Avg Loss: 24.7071\n",
            "Epoch 1, Batch 60, Avg Loss: 23.2611\n",
            "Epoch 1, Batch 70, Avg Loss: 22.3891\n",
            "Epoch 1, Batch 80, Avg Loss: 21.6588\n",
            "Epoch 1, Batch 90, Avg Loss: 20.9450\n",
            "Epoch 1, Batch 100, Avg Loss: 20.4959\n",
            "Epoch 1, Batch 110, Avg Loss: 19.8567\n",
            "Epoch 1, Batch 120, Avg Loss: 19.6249\n",
            "Epoch 1, Batch 130, Avg Loss: 19.2286\n",
            "Epoch 1, Batch 140, Avg Loss: 18.7564\n",
            "Epoch 1, Batch 150, Avg Loss: 18.2734\n",
            "Epoch 1, Batch 160, Avg Loss: 17.8654\n",
            "Epoch 1, Batch 170, Avg Loss: 17.5288\n",
            "Epoch 1, Batch 180, Avg Loss: 17.2822\n",
            "Epoch 1, Batch 190, Avg Loss: 16.9694\n",
            "Epoch 1, Batch 200, Avg Loss: 16.7779\n",
            "Epoch 1, Batch 210, Avg Loss: 16.4548\n",
            "Epoch 1, Batch 220, Avg Loss: 16.2362\n",
            "Epoch 1, Batch 230, Avg Loss: 15.9949\n",
            "Epoch 1, Batch 240, Avg Loss: 16.0833\n",
            "Epoch 1, Batch 250, Avg Loss: 15.8810\n",
            "Epoch 1, Batch 260, Avg Loss: 15.7052\n",
            "Epoch 1, Batch 270, Avg Loss: 15.5390\n",
            "Epoch 1, Batch 280, Avg Loss: 15.3348\n",
            "Epoch 1, Batch 290, Avg Loss: 15.1401\n",
            "Epoch 1, Batch 300, Avg Loss: 15.0736\n",
            "Epoch 1, Batch 310, Avg Loss: 14.9765\n",
            "Epoch 1, Batch 320, Avg Loss: 14.9161\n",
            "Epoch 1, Batch 330, Avg Loss: 14.7762\n",
            "\n",
            "Epoch 1 completed. Avg loss = 14.6601\n",
            "Epoch 2, Batch 10, Avg Loss: 8.9482\n",
            "Epoch 2, Batch 20, Avg Loss: 11.0266\n",
            "Epoch 2, Batch 30, Avg Loss: 11.2062\n",
            "Epoch 2, Batch 40, Avg Loss: 11.1834\n",
            "Epoch 2, Batch 50, Avg Loss: 10.8928\n",
            "Epoch 2, Batch 60, Avg Loss: 10.7822\n",
            "Epoch 2, Batch 70, Avg Loss: 11.0613\n",
            "Epoch 2, Batch 80, Avg Loss: 11.1560\n",
            "Epoch 2, Batch 90, Avg Loss: 11.1485\n",
            "Epoch 2, Batch 100, Avg Loss: 11.2964\n",
            "Epoch 2, Batch 110, Avg Loss: 11.2094\n",
            "Epoch 2, Batch 120, Avg Loss: 11.1664\n",
            "Epoch 2, Batch 130, Avg Loss: 11.0243\n",
            "Epoch 2, Batch 140, Avg Loss: 11.1644\n",
            "Epoch 2, Batch 150, Avg Loss: 11.2011\n",
            "Epoch 2, Batch 160, Avg Loss: 11.1725\n",
            "Epoch 2, Batch 170, Avg Loss: 11.1752\n",
            "Epoch 2, Batch 180, Avg Loss: 11.2105\n",
            "Epoch 2, Batch 190, Avg Loss: 11.1764\n",
            "Epoch 2, Batch 200, Avg Loss: 11.1767\n",
            "Epoch 2, Batch 210, Avg Loss: 11.1615\n",
            "Epoch 2, Batch 220, Avg Loss: 11.3425\n",
            "Epoch 2, Batch 230, Avg Loss: 11.3210\n",
            "Epoch 2, Batch 240, Avg Loss: 11.2283\n",
            "Epoch 2, Batch 250, Avg Loss: 11.1300\n",
            "Epoch 2, Batch 260, Avg Loss: 11.1045\n",
            "Epoch 2, Batch 270, Avg Loss: 11.0172\n",
            "Epoch 2, Batch 280, Avg Loss: 11.0327\n",
            "Epoch 2, Batch 290, Avg Loss: 11.0588\n",
            "Epoch 2, Batch 300, Avg Loss: 10.9968\n",
            "Epoch 2, Batch 310, Avg Loss: 10.9544\n",
            "Epoch 2, Batch 320, Avg Loss: 10.9326\n",
            "Epoch 2, Batch 330, Avg Loss: 11.0935\n",
            "\n",
            "Epoch 2 completed. Avg loss = 11.1124\n",
            "Epoch 3, Batch 10, Avg Loss: 14.3652\n",
            "Epoch 3, Batch 20, Avg Loss: 12.0074\n",
            "Epoch 3, Batch 30, Avg Loss: 11.3252\n",
            "Epoch 3, Batch 40, Avg Loss: 11.1803\n",
            "Epoch 3, Batch 50, Avg Loss: 10.9858\n",
            "Epoch 3, Batch 60, Avg Loss: 11.0741\n",
            "Epoch 3, Batch 70, Avg Loss: 10.9079\n",
            "Epoch 3, Batch 80, Avg Loss: 10.6002\n",
            "Epoch 3, Batch 90, Avg Loss: 10.8722\n",
            "Epoch 3, Batch 100, Avg Loss: 10.8019\n",
            "Epoch 3, Batch 110, Avg Loss: 10.8512\n",
            "Epoch 3, Batch 120, Avg Loss: 11.3962\n",
            "Epoch 3, Batch 130, Avg Loss: 11.3333\n",
            "Epoch 3, Batch 140, Avg Loss: 11.2108\n",
            "Epoch 3, Batch 150, Avg Loss: 11.1090\n",
            "Epoch 3, Batch 160, Avg Loss: 11.1441\n",
            "Epoch 3, Batch 170, Avg Loss: 11.1194\n",
            "Epoch 3, Batch 180, Avg Loss: 11.1038\n",
            "Epoch 3, Batch 190, Avg Loss: 11.1394\n",
            "Epoch 3, Batch 200, Avg Loss: 11.0455\n",
            "Epoch 3, Batch 210, Avg Loss: 11.0379\n",
            "Epoch 3, Batch 220, Avg Loss: 11.0645\n",
            "Epoch 3, Batch 230, Avg Loss: 11.0198\n",
            "Epoch 3, Batch 240, Avg Loss: 11.0240\n",
            "Epoch 3, Batch 250, Avg Loss: 10.9335\n",
            "Epoch 3, Batch 260, Avg Loss: 10.8804\n",
            "Epoch 3, Batch 270, Avg Loss: 10.9435\n",
            "Epoch 3, Batch 280, Avg Loss: 10.9530\n",
            "Epoch 3, Batch 290, Avg Loss: 10.9769\n",
            "Epoch 3, Batch 300, Avg Loss: 10.9594\n",
            "Epoch 3, Batch 310, Avg Loss: 10.9463\n",
            "Epoch 3, Batch 320, Avg Loss: 10.9345\n",
            "Epoch 3, Batch 330, Avg Loss: 10.8824\n",
            "\n",
            "Epoch 3 completed. Avg loss = 10.8837\n",
            "Epoch 4, Batch 10, Avg Loss: 9.6945\n",
            "Epoch 4, Batch 20, Avg Loss: 10.0375\n",
            "Epoch 4, Batch 30, Avg Loss: 9.9354\n",
            "Epoch 4, Batch 40, Avg Loss: 10.4519\n",
            "Epoch 4, Batch 50, Avg Loss: 10.6564\n",
            "Epoch 4, Batch 60, Avg Loss: 10.7341\n",
            "Epoch 4, Batch 70, Avg Loss: 10.6574\n",
            "Epoch 4, Batch 80, Avg Loss: 10.4518\n",
            "Epoch 4, Batch 90, Avg Loss: 10.5444\n",
            "Epoch 4, Batch 100, Avg Loss: 10.3764\n",
            "Epoch 4, Batch 110, Avg Loss: 10.2727\n",
            "Epoch 4, Batch 120, Avg Loss: 10.2361\n",
            "Epoch 4, Batch 130, Avg Loss: 10.3493\n",
            "Epoch 4, Batch 140, Avg Loss: 10.3744\n",
            "Epoch 4, Batch 150, Avg Loss: 10.4469\n",
            "Epoch 4, Batch 160, Avg Loss: 10.4535\n",
            "Epoch 4, Batch 170, Avg Loss: 10.7445\n",
            "Epoch 4, Batch 180, Avg Loss: 10.7174\n",
            "Epoch 4, Batch 190, Avg Loss: 10.7382\n",
            "Epoch 4, Batch 200, Avg Loss: 10.6820\n",
            "Epoch 4, Batch 210, Avg Loss: 10.6352\n",
            "Epoch 4, Batch 220, Avg Loss: 10.7088\n",
            "Epoch 4, Batch 230, Avg Loss: 10.6710\n",
            "Epoch 4, Batch 240, Avg Loss: 10.6632\n",
            "Epoch 4, Batch 250, Avg Loss: 10.5534\n",
            "Epoch 4, Batch 260, Avg Loss: 10.6019\n",
            "Epoch 4, Batch 270, Avg Loss: 10.6006\n",
            "Epoch 4, Batch 280, Avg Loss: 10.8727\n",
            "Epoch 4, Batch 290, Avg Loss: 10.8871\n",
            "Epoch 4, Batch 300, Avg Loss: 10.8570\n",
            "Epoch 4, Batch 310, Avg Loss: 10.8332\n",
            "Epoch 4, Batch 320, Avg Loss: 10.8124\n",
            "Epoch 4, Batch 330, Avg Loss: 10.7763\n",
            "\n",
            "Epoch 4 completed. Avg loss = 10.7338\n",
            "Epoch 5, Batch 10, Avg Loss: 8.7548\n",
            "Epoch 5, Batch 20, Avg Loss: 10.4208\n",
            "Epoch 5, Batch 30, Avg Loss: 10.2959\n",
            "Epoch 5, Batch 40, Avg Loss: 10.2260\n",
            "Epoch 5, Batch 50, Avg Loss: 10.4996\n",
            "Epoch 5, Batch 60, Avg Loss: 10.3455\n",
            "Epoch 5, Batch 70, Avg Loss: 10.4849\n",
            "Epoch 5, Batch 80, Avg Loss: 10.2534\n",
            "Epoch 5, Batch 90, Avg Loss: 10.3692\n",
            "Epoch 5, Batch 100, Avg Loss: 10.5139\n",
            "Epoch 5, Batch 110, Avg Loss: 10.4376\n",
            "Epoch 5, Batch 120, Avg Loss: 10.4311\n",
            "Epoch 5, Batch 130, Avg Loss: 11.4614\n",
            "Epoch 5, Batch 140, Avg Loss: 11.4575\n",
            "Epoch 5, Batch 150, Avg Loss: 11.4122\n",
            "Epoch 5, Batch 160, Avg Loss: 11.2732\n",
            "Epoch 5, Batch 170, Avg Loss: 11.2254\n",
            "Epoch 5, Batch 180, Avg Loss: 11.1781\n",
            "Epoch 5, Batch 190, Avg Loss: 11.1866\n",
            "Epoch 5, Batch 200, Avg Loss: 11.1842\n",
            "Epoch 5, Batch 210, Avg Loss: 11.1596\n",
            "Epoch 5, Batch 220, Avg Loss: 11.0813\n",
            "Epoch 5, Batch 230, Avg Loss: 10.9942\n",
            "Epoch 5, Batch 240, Avg Loss: 10.9503\n",
            "Epoch 5, Batch 250, Avg Loss: 10.9818\n",
            "Epoch 5, Batch 260, Avg Loss: 10.8584\n",
            "Epoch 5, Batch 270, Avg Loss: 10.7669\n",
            "Epoch 5, Batch 280, Avg Loss: 10.7456\n",
            "Epoch 5, Batch 290, Avg Loss: 10.7705\n",
            "Epoch 5, Batch 300, Avg Loss: 10.7559\n",
            "Epoch 5, Batch 310, Avg Loss: 10.8065\n",
            "Epoch 5, Batch 320, Avg Loss: 10.7686\n",
            "Epoch 5, Batch 330, Avg Loss: 10.7551\n",
            "\n",
            "Epoch 5 completed. Avg loss = 10.7236\n",
            "Epoch 6, Batch 10, Avg Loss: 12.0415\n",
            "Epoch 6, Batch 20, Avg Loss: 11.2012\n",
            "Epoch 6, Batch 30, Avg Loss: 11.3423\n",
            "Epoch 6, Batch 40, Avg Loss: 11.0864\n",
            "Epoch 6, Batch 50, Avg Loss: 10.5107\n",
            "Epoch 6, Batch 60, Avg Loss: 10.2222\n",
            "Epoch 6, Batch 70, Avg Loss: 11.0932\n",
            "Epoch 6, Batch 80, Avg Loss: 10.9612\n",
            "Epoch 6, Batch 90, Avg Loss: 10.8397\n",
            "Epoch 6, Batch 100, Avg Loss: 10.9430\n",
            "Epoch 6, Batch 110, Avg Loss: 10.8028\n",
            "Epoch 6, Batch 120, Avg Loss: 10.8695\n",
            "Epoch 6, Batch 130, Avg Loss: 10.6832\n",
            "Epoch 6, Batch 140, Avg Loss: 10.7036\n",
            "Epoch 6, Batch 150, Avg Loss: 10.7341\n",
            "Epoch 6, Batch 160, Avg Loss: 10.6001\n",
            "Epoch 6, Batch 170, Avg Loss: 10.6704\n",
            "Epoch 6, Batch 180, Avg Loss: 10.6526\n",
            "Epoch 6, Batch 190, Avg Loss: 10.7755\n",
            "Epoch 6, Batch 200, Avg Loss: 10.7252\n",
            "Epoch 6, Batch 210, Avg Loss: 10.6374\n",
            "Epoch 6, Batch 220, Avg Loss: 10.5951\n",
            "Epoch 6, Batch 230, Avg Loss: 10.7929\n",
            "Epoch 6, Batch 240, Avg Loss: 10.8861\n",
            "Epoch 6, Batch 250, Avg Loss: 10.8958\n",
            "Epoch 6, Batch 260, Avg Loss: 10.8704\n",
            "Epoch 6, Batch 270, Avg Loss: 10.8138\n",
            "Epoch 6, Batch 280, Avg Loss: 10.7469\n",
            "Epoch 6, Batch 290, Avg Loss: 10.6821\n",
            "Epoch 6, Batch 300, Avg Loss: 10.6578\n",
            "Epoch 6, Batch 310, Avg Loss: 10.6023\n",
            "Epoch 6, Batch 320, Avg Loss: 10.6180\n",
            "Epoch 6, Batch 330, Avg Loss: 10.6198\n",
            "\n",
            "Epoch 6 completed. Avg loss = 10.6483\n",
            "Epoch 7, Batch 10, Avg Loss: 10.0198\n",
            "Epoch 7, Batch 20, Avg Loss: 10.2198\n",
            "Epoch 7, Batch 30, Avg Loss: 9.4032\n",
            "Epoch 7, Batch 40, Avg Loss: 9.8612\n",
            "Epoch 7, Batch 50, Avg Loss: 10.0207\n",
            "Epoch 7, Batch 60, Avg Loss: 10.0678\n",
            "Epoch 7, Batch 70, Avg Loss: 11.4001\n",
            "Epoch 7, Batch 80, Avg Loss: 11.2397\n",
            "Epoch 7, Batch 90, Avg Loss: 11.1051\n",
            "Epoch 7, Batch 100, Avg Loss: 11.4631\n",
            "Epoch 7, Batch 110, Avg Loss: 11.4251\n",
            "Epoch 7, Batch 120, Avg Loss: 11.3511\n",
            "Epoch 7, Batch 130, Avg Loss: 11.1801\n",
            "Epoch 7, Batch 140, Avg Loss: 11.0450\n",
            "Epoch 7, Batch 150, Avg Loss: 10.8651\n",
            "Epoch 7, Batch 160, Avg Loss: 10.7931\n",
            "Epoch 7, Batch 170, Avg Loss: 10.9057\n",
            "Epoch 7, Batch 180, Avg Loss: 10.7854\n",
            "Epoch 7, Batch 190, Avg Loss: 10.8877\n",
            "Epoch 7, Batch 200, Avg Loss: 10.8445\n",
            "Epoch 7, Batch 210, Avg Loss: 10.8893\n",
            "Epoch 7, Batch 220, Avg Loss: 10.9051\n",
            "Epoch 7, Batch 230, Avg Loss: 10.8550\n",
            "Epoch 7, Batch 240, Avg Loss: 10.8270\n",
            "Epoch 7, Batch 250, Avg Loss: 10.8941\n",
            "Epoch 7, Batch 260, Avg Loss: 10.9090\n",
            "Epoch 7, Batch 270, Avg Loss: 10.8548\n",
            "Epoch 7, Batch 280, Avg Loss: 10.8231\n",
            "Epoch 7, Batch 290, Avg Loss: 10.7947\n",
            "Epoch 7, Batch 300, Avg Loss: 10.7846\n",
            "Epoch 7, Batch 310, Avg Loss: 10.7362\n",
            "Epoch 7, Batch 320, Avg Loss: 10.6808\n",
            "Epoch 7, Batch 330, Avg Loss: 10.6758\n",
            "\n",
            "Epoch 7 completed. Avg loss = 10.6554\n",
            "Epoch 8, Batch 10, Avg Loss: 9.8076\n",
            "Epoch 8, Batch 20, Avg Loss: 10.6866\n",
            "Epoch 8, Batch 30, Avg Loss: 11.0668\n",
            "Epoch 8, Batch 40, Avg Loss: 10.6408\n",
            "Epoch 8, Batch 50, Avg Loss: 10.7250\n",
            "Epoch 8, Batch 60, Avg Loss: 10.4748\n",
            "Epoch 8, Batch 70, Avg Loss: 10.4864\n",
            "Epoch 8, Batch 80, Avg Loss: 10.6468\n",
            "Epoch 8, Batch 90, Avg Loss: 10.5182\n",
            "Epoch 8, Batch 100, Avg Loss: 10.3883\n",
            "Epoch 8, Batch 110, Avg Loss: 10.4667\n",
            "Epoch 8, Batch 120, Avg Loss: 10.4858\n",
            "Epoch 8, Batch 130, Avg Loss: 10.5465\n",
            "Epoch 8, Batch 140, Avg Loss: 11.0174\n",
            "Epoch 8, Batch 150, Avg Loss: 10.9619\n",
            "Epoch 8, Batch 160, Avg Loss: 10.9518\n",
            "Epoch 8, Batch 170, Avg Loss: 10.8667\n",
            "Epoch 8, Batch 180, Avg Loss: 11.0103\n",
            "Epoch 8, Batch 190, Avg Loss: 11.0092\n",
            "Epoch 8, Batch 200, Avg Loss: 11.0165\n",
            "Epoch 8, Batch 210, Avg Loss: 10.9349\n",
            "Epoch 8, Batch 220, Avg Loss: 10.8521\n",
            "Epoch 8, Batch 230, Avg Loss: 10.7706\n",
            "Epoch 8, Batch 240, Avg Loss: 10.7253\n",
            "Epoch 8, Batch 250, Avg Loss: 10.7525\n",
            "Epoch 8, Batch 260, Avg Loss: 10.7842\n",
            "Epoch 8, Batch 270, Avg Loss: 10.7231\n",
            "Epoch 8, Batch 280, Avg Loss: 10.6960\n",
            "Epoch 8, Batch 290, Avg Loss: 10.6700\n",
            "Epoch 8, Batch 300, Avg Loss: 10.6719\n",
            "Epoch 8, Batch 310, Avg Loss: 10.6832\n",
            "Epoch 8, Batch 320, Avg Loss: 10.6107\n",
            "Epoch 8, Batch 330, Avg Loss: 10.5583\n",
            "\n",
            "Epoch 8 completed. Avg loss = 10.5686\n",
            "Epoch 9, Batch 10, Avg Loss: 11.9079\n",
            "Epoch 9, Batch 20, Avg Loss: 14.2211\n",
            "Epoch 9, Batch 30, Avg Loss: 13.4979\n",
            "Epoch 9, Batch 40, Avg Loss: 12.7168\n",
            "Epoch 9, Batch 50, Avg Loss: 11.8697\n",
            "Epoch 9, Batch 60, Avg Loss: 11.6107\n",
            "Epoch 9, Batch 70, Avg Loss: 11.2948\n",
            "Epoch 9, Batch 80, Avg Loss: 11.2389\n",
            "Epoch 9, Batch 90, Avg Loss: 10.9884\n",
            "Epoch 9, Batch 100, Avg Loss: 10.7898\n",
            "Epoch 9, Batch 110, Avg Loss: 10.5994\n",
            "Epoch 9, Batch 120, Avg Loss: 10.5807\n",
            "Epoch 9, Batch 130, Avg Loss: 10.7223\n",
            "Epoch 9, Batch 140, Avg Loss: 10.7206\n",
            "Epoch 9, Batch 150, Avg Loss: 10.8930\n",
            "Epoch 9, Batch 160, Avg Loss: 10.9000\n",
            "Epoch 9, Batch 170, Avg Loss: 10.7828\n",
            "Epoch 9, Batch 180, Avg Loss: 10.7437\n",
            "Epoch 9, Batch 190, Avg Loss: 10.5988\n",
            "Epoch 9, Batch 200, Avg Loss: 10.6530\n",
            "Epoch 9, Batch 210, Avg Loss: 10.7040\n",
            "Epoch 9, Batch 220, Avg Loss: 10.7287\n",
            "Epoch 9, Batch 230, Avg Loss: 10.6789\n",
            "Epoch 9, Batch 240, Avg Loss: 10.6141\n",
            "Epoch 9, Batch 250, Avg Loss: 10.6601\n",
            "Epoch 9, Batch 260, Avg Loss: 10.6712\n",
            "Epoch 9, Batch 270, Avg Loss: 10.7296\n",
            "Epoch 9, Batch 280, Avg Loss: 10.6771\n",
            "Epoch 9, Batch 290, Avg Loss: 10.7075\n",
            "Epoch 9, Batch 300, Avg Loss: 10.7149\n",
            "Epoch 9, Batch 310, Avg Loss: 10.6880\n",
            "Epoch 9, Batch 320, Avg Loss: 10.6520\n",
            "Epoch 9, Batch 330, Avg Loss: 10.6742\n",
            "\n",
            "Epoch 9 completed. Avg loss = 10.6175\n",
            "Epoch 10, Batch 10, Avg Loss: 12.9600\n",
            "Epoch 10, Batch 20, Avg Loss: 11.1958\n",
            "Epoch 10, Batch 30, Avg Loss: 10.4783\n",
            "Epoch 10, Batch 40, Avg Loss: 11.1591\n",
            "Epoch 10, Batch 50, Avg Loss: 10.9310\n",
            "Epoch 10, Batch 60, Avg Loss: 10.5390\n",
            "Epoch 10, Batch 70, Avg Loss: 10.4049\n",
            "Epoch 10, Batch 80, Avg Loss: 10.6836\n",
            "Epoch 10, Batch 90, Avg Loss: 10.5058\n",
            "Epoch 10, Batch 100, Avg Loss: 10.5710\n",
            "Epoch 10, Batch 110, Avg Loss: 10.3806\n",
            "Epoch 10, Batch 120, Avg Loss: 10.2633\n",
            "Epoch 10, Batch 130, Avg Loss: 10.4793\n",
            "Epoch 10, Batch 140, Avg Loss: 10.4246\n",
            "Epoch 10, Batch 150, Avg Loss: 10.3765\n",
            "Epoch 10, Batch 160, Avg Loss: 10.3773\n",
            "Epoch 10, Batch 170, Avg Loss: 10.4062\n",
            "Epoch 10, Batch 180, Avg Loss: 10.4010\n",
            "Epoch 10, Batch 190, Avg Loss: 10.3959\n",
            "Epoch 10, Batch 200, Avg Loss: 10.3971\n",
            "Epoch 10, Batch 210, Avg Loss: 10.4067\n",
            "Epoch 10, Batch 220, Avg Loss: 10.4841\n",
            "Epoch 10, Batch 230, Avg Loss: 10.4297\n",
            "Epoch 10, Batch 240, Avg Loss: 10.3170\n",
            "Epoch 10, Batch 250, Avg Loss: 10.3832\n",
            "Epoch 10, Batch 260, Avg Loss: 10.3873\n",
            "Epoch 10, Batch 270, Avg Loss: 10.4411\n",
            "Epoch 10, Batch 280, Avg Loss: 10.4421\n",
            "Epoch 10, Batch 290, Avg Loss: 10.4546\n",
            "Epoch 10, Batch 300, Avg Loss: 10.4274\n",
            "Epoch 10, Batch 310, Avg Loss: 10.4028\n",
            "Epoch 10, Batch 320, Avg Loss: 10.3610\n",
            "Epoch 10, Batch 330, Avg Loss: 10.3420\n",
            "\n",
            "Epoch 10 completed. Avg loss = 10.5776\n",
            "Model saved to MLP.pth\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (network): Sequential(\n",
              "    (0): Linear(in_features=13, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.2, inplace=False)\n",
              "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.2, inplace=False)\n",
              "    (6): Linear(in_features=128, out_features=29, bias=True)\n",
              "  )\n",
              "  (loss_fn): CTCLoss()\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# MLP\n",
        "train_model(batch_size=8, num_workers=2, save_path=f'{MODEL_TYPE}.pth', model_type=f'{MODEL_TYPE}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fL52LXl-IIti",
        "outputId": "95d654bb-8eb1-4940-d0f0-ff415d288d2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Using device: cuda\n",
            "Initializing MLP model...\n",
            "Loading model from MLP.pth\n",
            "\n",
            "Evaluating on device: cuda\n",
            "\n",
            "================================================================================\n",
            "EVALUATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "Sample 1:\n",
            "  Reference:  he hoped there would be stew for dinner turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour fattened sauce\n",
            "  Predicted:  l\n",
            "  WER: 560.71%, CER: 99.37%\n",
            "\n",
            "Sample 2:\n",
            "  Reference:  stuff it into you his belly counselled him\n",
            "  Predicted:  l\n",
            "  WER: 512.50%, CER: 97.62%\n",
            "\n",
            "Sample 3:\n",
            "  Reference:  after early nightfall the yellow lamps would light up here and there the squalid quarter of the brothels\n",
            "  Predicted:  l\n",
            "  WER: 572.22%, CER: 99.04%\n",
            "\n",
            "Sample 4:\n",
            "  Reference:  hello bertie any good in your mind\n",
            "  Predicted:  l\n",
            "  WER: 471.43%, CER: 97.06%\n",
            "\n",
            "Sample 5:\n",
            "  Reference:  number ten fresh nelly is waiting on you good night husband\n",
            "  Predicted:  l\n",
            "  WER: 527.27%, CER: 98.31%\n",
            "\n",
            "================================================================================\n",
            "AVERAGE METRICS (over 20 samples)\n",
            "  Average WER: 528.66%\n",
            "  Average CER: 98.72%\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(5.286643914802452, 0.9872262922764088)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_and_evaluate(\n",
        "    model_path=f'{MODEL_TYPE}.pth',\n",
        "    model_type=f'{MODEL_TYPE}',\n",
        "    num_samples=20\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEmQ0FG6T43L"
      },
      "source": [
        "# Part 5 : Optuna Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VqFwsSelWiXI"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to handle variable-length audio\"\"\"\n",
        "    waveforms = [item[0] for item in batch]\n",
        "    sample_rates = [item[1] for item in batch]\n",
        "    transcripts = [item[2] for item in batch]\n",
        "\n",
        "    # We only need these 3 for training/tuning\n",
        "    return waveforms, sample_rates, transcripts\n",
        "\n",
        "def get_data_loaders(batch_size=8, num_workers=2):\n",
        "    \"\"\"Downloads (if needed) and splits the dataset\"\"\"\n",
        "    # Using dev-clean for tuning speed.\n",
        "    os.makedirs(\"./data\", exist_ok=True)\n",
        "    dataset = LIBRISPEECH(\"./data\", url=\"dev-clean\", download=True)\n",
        "\n",
        "    # Split: 80% Train, 20% Validation\n",
        "    # We use a smaller subset for hyperparameter tuning to save time\n",
        "    # (e.g., using only 50% of the data to find good params quickly)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_set, val_set = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_set, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=num_workers, collate_fn=collate_fn\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_set, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, collate_fn=collate_fn\n",
        "    )\n",
        "    return train_loader, val_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dCBaXSAYWn3_"
      },
      "outputs": [],
      "source": [
        "class TunableGRU(nn.Module):\n",
        "    def __init__(self, input_dim=40, hidden_dim=128, num_layers=2, dropout=0.1, n_classes=29):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0\n",
        "        )\n",
        "\n",
        "        # Hidden dim * 2 because of bidirectional\n",
        "        self.classifier = nn.Linear(hidden_dim * 2, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [Batch, Time, Features]\n",
        "        out, _ = self.gru(x)\n",
        "        logits = self.classifier(out)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpR7LRP-Sdsq"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    # --- A. Device Setup ---\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # --- B. Hyperparameter Suggestion ---\n",
        "    # Model Params\n",
        "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [128, 256, 512])\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
        "\n",
        "    # Training Params\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16])\n",
        "\n",
        "    # --- C. Data & Transform Setup ---\n",
        "    train_loader, val_loader = get_data_loaders(batch_size=batch_size)\n",
        "\n",
        "    melspec_transform = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=16000,\n",
        "        n_mels=40,\n",
        "        n_fft=400,\n",
        "        hop_length=160\n",
        "    ).to(device)\n",
        "\n",
        "    char2idx = {c: i for i, c in enumerate(\"abcdefghijklmnopqrstuvwxyz '\")}\n",
        "\n",
        "    # --- D. Model & Optimizer ---\n",
        "    model = TunableGRU(\n",
        "        input_dim=40,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout,\n",
        "        n_classes=29\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    ctc_loss = nn.CTCLoss(blank=28, zero_infinity=True)\n",
        "\n",
        "    # --- E. Training Loop (Shortened for Tuning) ---\n",
        "    # We train for fewer epochs (e.g., 5) just to check convergence speed\n",
        "    num_epochs = 5\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "\n",
        "        for batch_idx, (waveforms, _, transcripts) in enumerate(train_loader):\n",
        "            # 1. Feature Extraction\n",
        "            specs = []\n",
        "            targets = []\n",
        "            target_lengths = []\n",
        "\n",
        "            for i, waveform in enumerate(waveforms):\n",
        "                # Move to GPU *before* transform to avoid device mismatch\n",
        "                waveform = waveform.to(device)\n",
        "\n",
        "                # Transform\n",
        "                spec = melspec_transform(waveform) # [1, n_mels, time]\n",
        "                spec = spec.squeeze(0).transpose(0, 1) # [time, n_mels]\n",
        "\n",
        "                # STABILIZATION: Log & Normalize\n",
        "                spec = spec.clamp(min=1e-9).log2()\n",
        "                spec = (spec - spec.mean()) / (spec.std() + 1e-5)\n",
        "\n",
        "                specs.append(spec)\n",
        "\n",
        "                # Targets\n",
        "                t_idx = [char2idx[c] for c in transcripts[i].lower() if c in char2idx]\n",
        "                targets.extend(t_idx)\n",
        "                target_lengths.append(len(t_idx))\n",
        "\n",
        "            if not specs: continue\n",
        "\n",
        "            # Pad sequences\n",
        "            specs_padded = nn.utils.rnn.pad_sequence(specs, batch_first=True).to(device)\n",
        "            input_lengths = torch.tensor([s.size(0) for s in specs], dtype=torch.long, device=device)\n",
        "\n",
        "            targets_tensor = torch.tensor(targets, dtype=torch.long, device=device)\n",
        "            target_lengths_tensor = torch.tensor(target_lengths, dtype=torch.long, device=device)\n",
        "\n",
        "            # 2. Forward Pass\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(specs_padded) # [B, T, Classes]\n",
        "\n",
        "            # CTC expects [T, B, Classes] + LogSoftmax\n",
        "            log_probs = F.log_softmax(logits, dim=2).transpose(0, 1)\n",
        "\n",
        "            # 3. Loss & Backprop\n",
        "            loss = ctc_loss(log_probs, targets_tensor, input_lengths, target_lengths_tensor)\n",
        "\n",
        "            if torch.isnan(loss):\n",
        "                # Prune trials that explode immediately\n",
        "                raise optuna.TrialPruned()\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip gradients\n",
        "            optimizer.step()\n",
        "\n",
        "        # --- F. Validation Step (End of Epoch) ---\n",
        "        model.eval()\n",
        "        val_loss_accum = 0.0\n",
        "        val_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for waveforms, _, transcripts in val_loader:\n",
        "                specs = []\n",
        "                targets = []\n",
        "                target_lengths = []\n",
        "\n",
        "                for i, waveform in enumerate(waveforms):\n",
        "                    waveform = waveform.to(device)\n",
        "                    spec = melspec_transform(waveform).squeeze(0).transpose(0, 1)\n",
        "                    spec = spec.clamp(min=1e-9).log2()\n",
        "                    spec = (spec - spec.mean()) / (spec.std() + 1e-5)\n",
        "                    specs.append(spec)\n",
        "\n",
        "                    t_idx = [char2idx[c] for c in transcripts[i].lower() if c in char2idx]\n",
        "                    targets.extend(t_idx)\n",
        "                    target_lengths.append(len(t_idx))\n",
        "\n",
        "                if not specs: continue\n",
        "\n",
        "                specs_padded = nn.utils.rnn.pad_sequence(specs, batch_first=True).to(device)\n",
        "                input_lengths = torch.tensor([s.size(0) for s in specs], dtype=torch.long, device=device)\n",
        "                targets_tensor = torch.tensor(targets, dtype=torch.long, device=device)\n",
        "                target_lengths_tensor = torch.tensor(target_lengths, dtype=torch.long, device=device)\n",
        "\n",
        "                logits = model(specs_padded)\n",
        "                log_probs = F.log_softmax(logits, dim=2).transpose(0, 1)\n",
        "\n",
        "                loss = ctc_loss(log_probs, targets_tensor, input_lengths, target_lengths_tensor)\n",
        "                if not torch.isnan(loss) and not torch.isinf(loss):\n",
        "                    val_loss_accum += loss.item()\n",
        "                    val_batches += 1\n",
        "\n",
        "        avg_val_loss = val_loss_accum / val_batches if val_batches > 0 else float('inf')\n",
        "\n",
        "        # --- G. Reporting & Pruning ---\n",
        "        trial.report(avg_val_loss, epoch)\n",
        "\n",
        "        if trial.should_prune():\n",
        "            print(f\"Trial {trial.number} pruned at epoch {epoch} with loss {avg_val_loss:.4f}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    return avg_val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odm612SJXsgX",
        "outputId": "eed38927-9b0f-4d9e-a976-f126f70bce7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-24 10:21:26,447] A new study created in memory with name: no-name-b6db3b8c-73cc-49a5-8e54-a21d57285cae\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Optuna Study...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-24 10:24:08,331] Trial 0 finished with value: 1.7167845456039204 and parameters: {'hidden_dim': 256, 'num_layers': 1, 'dropout': 0.3406807446255472, 'lr': 0.0006922706622600342, 'batch_size': 8}. Best is trial 0 with value: 1.7167845456039204.\n",
            "[I 2025-11-24 10:29:13,853] Trial 1 finished with value: 1.2736535282695995 and parameters: {'hidden_dim': 256, 'num_layers': 3, 'dropout': 0.4406820799990565, 'lr': 0.0011901830103776144, 'batch_size': 16}. Best is trial 1 with value: 1.2736535282695995.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting Optuna Study...\")\n",
        "\n",
        "    # TPESampler is efficient at finding good regions\n",
        "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler())\n",
        "\n",
        "    # Run 10-20 trials\n",
        "    study.optimize(objective, n_trials=10)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"HYPERPARAMETER TUNING COMPLETE\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Best Validation Loss: {study.best_value:.4f}\")\n",
        "    print(\"Best Hyperparameters:\")\n",
        "    for key, value in study.best_params.items():\n",
        "        print(f\"  {key}: {value}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
