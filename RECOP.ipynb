{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMSDShfpngMlmBdPZFyhXBO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milik0/speech-recognition/blob/main/RECOP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_itZN1ThqOKw"
      },
      "outputs": [],
      "source": [
        "# Reconnaissance Automatique de la Parole - Jour 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "import torch\n",
        "from torchaudio.datasets import LIBRISPEECH\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "S1edRcGnqlw8"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmNHF6KG7qMl",
        "outputId": "7472559e-2371-475f-c49f-867a588fae49"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Levenshtein in /usr/local/lib/python3.12/dist-packages (0.27.3)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from Levenshtein) (3.14.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def manage_data(batch_size, num_workers):\n",
        "    train_dataset = LIBRISPEECH(\"./data\", url=\"dev-clean\", download=True)\n",
        "    print(\"Data Downloaded !\")\n",
        "\n",
        "    print(f\"Dataset size: {len(train_dataset)} samples\")\n",
        "    print(f\"First sample shape: {train_dataset[0][0].shape}\")\n",
        "\n",
        "    # Create DataLoader for batch processing\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "    return train_loader\n",
        "\n",
        "def load_data():\n",
        "    print(\"Downloading data\")\n",
        "\n",
        "    os.makedirs(\"data\", exist_ok=True)\n",
        "    # Use 'dev-clean' for testing, 'train-clean-100' for training\n",
        "    train_loader = manage_data(batch_size=8, num_workers=2)\n",
        "\n",
        "load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFSDfq7Uqvsk",
        "outputId": "4488129b-981d-455e-fe67-2c65e9a911d2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data\n",
            "Data Downloaded !\n",
            "Dataset size: 2703 samples\n",
            "First sample shape: torch.Size([1, 93680])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -----------------------------\n",
        "# DEVICE MANAGEMENT\n",
        "# -----------------------------\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"Using CUDA GPU\")\n",
        "        return torch.device(\"cuda\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        print(\"Using Apple MPS\")\n",
        "        return torch.device(\"mps\")\n",
        "    else:\n",
        "        print(\"Using CPU\")\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "device = get_device()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5rXVNS-9oXZ",
        "outputId": "1b3aef78-d631-4618-d43f-161066faaa25"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "from torchaudio import transforms\n",
        "\n",
        "def _MFCC(waveform, sample_rate):\n",
        "    transform = transforms.MFCC(\n",
        "        sample_rate=sample_rate,\n",
        "        n_mfcc=13,\n",
        "        melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 23, \"center\": False}\n",
        "    )\n",
        "    mfcc = transform(waveform)\n",
        "    return mfcc"
      ],
      "metadata": {
        "id": "304W6VXs8jVs"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "2vTkVfvKJEB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size=13, hidden_size=256, output_size=29, num_layers=3):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(input_size, hidden_size))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Dropout(0.2))\n",
        "\n",
        "        for _ in range(num_layers - 2):\n",
        "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.2))\n",
        "\n",
        "        layers.append(nn.Linear(hidden_size, output_size))\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "        self.loss_fn = nn.CTCLoss(blank=28, zero_infinity=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "    def loss(self, log_probs, targets, input_lengths, target_lengths):\n",
        "        return self.loss_fn(log_probs, targets, input_lengths, target_lengths)"
      ],
      "metadata": {
        "id": "k68rzOJKq-61"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, n_classes=30, n_mels=40):  # 30 phonemes or characters\n",
        "        super().__init__()\n",
        "\n",
        "        # 1D convolutions along time axis\n",
        "        # Input channels = n_mels (frequency bins treated as channels)\n",
        "        self.conv1 = nn.Conv1d(n_mels, 128, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool1d(2)  # pool along time axis\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # final classifier\n",
        "        self.classifier = nn.Linear(512, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, 1, F, T] from spectrogram (mel_spec gives [1, n_mels, time])\n",
        "        # Reshape to [B, F, T] for 1D conv along time\n",
        "        if x.dim() == 4:\n",
        "            # Input is [B, 1, F, T]\n",
        "            x = x.squeeze(1)  # -> [B, F, T]\n",
        "        elif x.dim() == 3:\n",
        "            # Input is already [B, F, T] or needs transpose\n",
        "            # Check if last dim is n_mels (needs transpose)\n",
        "            if x.shape[-1] == self.conv1.in_channels:\n",
        "                x = x.transpose(1, 2)  # [B, T, F] -> [B, F, T]\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)  # -> [B, 128, T/2]\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)  # -> [B, 256, T/4]\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x)  # -> [B, 512, T/8]\n",
        "\n",
        "        # Transpose to [B, T', C] for time-distributed classification\n",
        "        x = x.transpose(1, 2)  # -> [B, T', 512]\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        out = self.classifier(x)  # [B, T', n_classes]\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "bC4HQN0-JOr6"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_dim=40, hidden_dim=128, num_layers=2, n_classes=30):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_dim * 2, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, F]\n",
        "        out, _ = self.gru(x)       # out: [B, T, hidden*2]\n",
        "        logits = self.classifier(out)\n",
        "        return logits              # [B, T, n_classes]\n"
      ],
      "metadata": {
        "id": "8H0KjInXPOGc"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, d_model]\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, input_dim=40, d_model=256, nhead=8, num_layers=6, dim_feedforward=1024, n_classes=29, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Transformer-based speech recognition model\n",
        "\n",
        "        Args:\n",
        "            input_dim: Input feature dimension (e.g., 40 for mel spectrograms)\n",
        "            d_model: Dimension of the model\n",
        "            nhead: Number of attention heads\n",
        "            num_layers: Number of transformer encoder layers\n",
        "            dim_feedforward: Dimension of feedforward network\n",
        "            n_classes: Number of output classes\n",
        "            dropout: Dropout rate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Input projection\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Output classifier\n",
        "        self.classifier = nn.Linear(d_model, n_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, F] where F is input_dim (e.g., 40 mel bins)\n",
        "\n",
        "        # Project to d_model\n",
        "        x = self.input_projection(x)  # [B, T, d_model]\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Transformer encoding\n",
        "        x = self.transformer_encoder(x)  # [B, T, d_model]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(x)  # [B, T, n_classes]\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "MfPpkst7XyT4"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "rf2ew4X6JFYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "import torch\n",
        "from torchaudio.datasets import LIBRISPEECH\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Model selection parameter - change this to 'MLP', 'CNN', 'GRU', or 'Transformer'\n",
        "MODEL_TYPE = 'Transformer'  # Options: 'MLP', 'CNN', 'GRU', 'Transformer'\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to handle variable-length audio\"\"\"\n",
        "    waveforms = [item[0] for item in batch]\n",
        "    sample_rates = [item[1] for item in batch]\n",
        "    transcripts = [item[2] for item in batch]\n",
        "    speaker_ids = [item[3] for item in batch]\n",
        "    chapter_ids = [item[4] for item in batch]\n",
        "    utterance_ids = [item[5] for item in batch]\n",
        "\n",
        "    return waveforms, sample_rates, transcripts, speaker_ids, chapter_ids, utterance_ids\n",
        "\n",
        "def manage_data(batch_size, num_workers):\n",
        "    train_dataset = LIBRISPEECH(\"./data\", url=\"train-clean-100\", download=True)\n",
        "    print(\"Data Downloaded !\")\n",
        "\n",
        "    print(f\"Dataset size: {len(train_dataset)} samples\")\n",
        "    print(f\"First sample shape: {train_dataset[0][0].shape}\")\n",
        "\n",
        "    # Create DataLoader for batch processing\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "    return train_loader\n",
        "\n",
        "def train_model(batch_size, num_workers, num_epochs=2, save_path='model_checkpoint.pth', model_type=MODEL_TYPE):\n",
        "\n",
        "    # -------------------------------\n",
        "    # GPU / MPS / CPU management\n",
        "    # -------------------------------\n",
        "    device = torch.device(\n",
        "        \"cuda\" if torch.cuda.is_available()\n",
        "        else \"mps\" if torch.backends.mps.is_available()\n",
        "        else \"cpu\"\n",
        "    )\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    train_loader = manage_data(batch_size, num_workers)\n",
        "\n",
        "    # Initialize model based on type\n",
        "    if model_type == 'MLP':\n",
        "        model = MLP(input_size=13, hidden_size=128, output_size=29)\n",
        "        print(\"Using MLP\")\n",
        "    elif model_type == 'CNN':\n",
        "        model = CNN(n_classes=29, n_mels=40)\n",
        "        print(\"Using CNN\")\n",
        "    elif model_type == 'GRU':\n",
        "        model = GRU(input_dim=40, hidden_dim=128, num_layers=2, n_classes=29)\n",
        "        print(\"Using GRU\")\n",
        "    elif model_type == 'Transformer':\n",
        "        model = Transformer(input_dim=40, d_model=256, nhead=8, num_layers=6, n_classes=29)\n",
        "        print(\"Using Transformer\")\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model_type\")\n",
        "\n",
        "    # Move model to GPU/MPS/CPU\n",
        "    model = model.to(device)\n",
        "    model.train(True)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "    ctc_loss = torch.nn.CTCLoss(blank=28, zero_infinity=True)\n",
        "\n",
        "    char2idx = {c: i for i, c in enumerate(\"abcdefghijklmnopqrstuvwxyz '\")}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, (waveforms, sample_rates, transcripts, _, _, _) in enumerate(train_loader):\n",
        "\n",
        "            batch_log_probs = []\n",
        "            batch_targets = []\n",
        "            batch_input_lengths = []\n",
        "            batch_target_lengths = []\n",
        "\n",
        "            for i, (waveform, sample_rate) in enumerate(zip(waveforms, sample_rates)):\n",
        "\n",
        "                # Move waveform to device\n",
        "                waveform = waveform.to(device)\n",
        "\n",
        "                # -------------------------------\n",
        "                # Feature extraction (on device)\n",
        "                # -------------------------------\n",
        "                if model_type == 'MLP':\n",
        "                    features = _MFCC(waveform, sample_rate)  # should return on same device\n",
        "                    features = features.squeeze(0).transpose(0, 1)\n",
        "\n",
        "                elif model_type == 'CNN':\n",
        "                    mel_spec_transform = torchaudio.transforms.MelSpectrogram(\n",
        "                        sample_rate=sample_rate,\n",
        "                        n_mels=40,\n",
        "                        n_fft=400,\n",
        "                        hop_length=80\n",
        "                    ).to(device)\n",
        "\n",
        "                    features = mel_spec_transform(waveform)\n",
        "                    features = features.clamp(min=1e-9).log2()\n",
        "                    features = features.unsqueeze(0)\n",
        "\n",
        "                elif model_type == 'GRU':\n",
        "                    mel_spec_transform = torchaudio.transforms.MelSpectrogram(\n",
        "                        sample_rate=sample_rate,\n",
        "                        n_mels=40,\n",
        "                        n_fft=400,\n",
        "                        hop_length=80\n",
        "                    ).to(device)\n",
        "\n",
        "                    features = mel_spec_transform(waveform)\n",
        "                    features = features.clamp(min=1e-9).log2()\n",
        "                    # GRU expects [B, T, F] -> transpose\n",
        "                    features = features.squeeze(0).transpose(0, 1).unsqueeze(0)  # [1, T, 40]\n",
        "\n",
        "                elif model_type == 'Transformer':\n",
        "                    mel_spec_transform = torchaudio.transforms.MelSpectrogram(\n",
        "                        sample_rate=sample_rate,\n",
        "                        n_mels=40,\n",
        "                        n_fft=400,\n",
        "                        hop_length=80\n",
        "                    ).to(device)\n",
        "\n",
        "                    features = mel_spec_transform(waveform)\n",
        "                    features = features.clamp(min=1e-9).log2()\n",
        "                    # Transformer expects [B, T, F]\n",
        "                    features = features.squeeze(0).transpose(0, 1).unsqueeze(0)  # [1, T, 40]\n",
        "\n",
        "                # Target on device\n",
        "                target = torch.tensor(\n",
        "                    [char2idx[c] for c in transcripts[i].lower() if c in char2idx],\n",
        "                    dtype=torch.long,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                # Length checks\n",
        "                if model_type == 'MLP':\n",
        "                    input_len = features.size(0)\n",
        "                elif model_type == 'CNN':\n",
        "                    input_len = features.size(-1) // 8\n",
        "                elif model_type == 'GRU':\n",
        "                    input_len = features.size(1)  # Time dimension for GRU\n",
        "                elif model_type == 'Transformer':\n",
        "                    input_len = features.size(1)  # Time dimension for Transformer\n",
        "\n",
        "                if input_len <= len(target):\n",
        "                    print(f\"Skipping sample {i}: input_len={input_len}, target_len={len(target)}\")\n",
        "                    continue\n",
        "\n",
        "                # Forward\n",
        "                logits = model(features)\n",
        "                if model_type == 'CNN':\n",
        "                    logits = logits.squeeze(0)\n",
        "                elif model_type == 'GRU':\n",
        "                    logits = logits.squeeze(0)\n",
        "                elif model_type == 'Transformer':\n",
        "                    logits = logits.squeeze(0)\n",
        "\n",
        "                log_probs = F.log_softmax(logits, dim=1)\n",
        "\n",
        "                batch_log_probs.append(log_probs)\n",
        "                batch_targets.append(target)\n",
        "                batch_input_lengths.append(log_probs.size(0))\n",
        "                batch_target_lengths.append(len(target))\n",
        "\n",
        "            if len(batch_log_probs) == 0:\n",
        "                continue\n",
        "\n",
        "            # -------------------------------\n",
        "            # Pad sequences (on device)\n",
        "            # -------------------------------\n",
        "            max_input_len = max(batch_input_lengths)\n",
        "            padded_log_probs = torch.zeros(\n",
        "                max_input_len, len(batch_log_probs), 29,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            for i, log_probs in enumerate(batch_log_probs):\n",
        "                padded_log_probs[:log_probs.size(0), i, :] = log_probs\n",
        "\n",
        "            concatenated_targets = torch.cat(batch_targets)\n",
        "            input_lengths = torch.tensor(batch_input_lengths, dtype=torch.long, device=device)\n",
        "            target_lengths = torch.tensor(batch_target_lengths, dtype=torch.long, device=device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = ctc_loss(padded_log_probs, concatenated_targets, input_lengths, target_lengths)\n",
        "\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"NaN loss detected at batch {batch_idx}\")\n",
        "                continue\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            if (batch_idx + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_idx+1}, Avg Loss: {total_loss/num_batches:.4f}\")\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1} completed. Avg loss = {total_loss/num_batches:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Model saved to {save_path}\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "hzFZGDOv8wnG"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "qS0mNQTwJG3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torchaudio.datasets import LIBRISPEECH\n",
        "from torch.utils.data import DataLoader\n",
        "import Levenshtein\n",
        "\n",
        "\n",
        "def decode_predictions(log_probs, idx2char, blank_idx=28):\n",
        "    predictions = torch.argmax(log_probs, dim=1)\n",
        "\n",
        "    decoded = []\n",
        "    prev_char = None\n",
        "    for pred in predictions:\n",
        "        pred_idx = pred.item()\n",
        "        if pred_idx != blank_idx and pred_idx != prev_char:\n",
        "            decoded.append(idx2char[pred_idx])\n",
        "        prev_char = pred_idx\n",
        "    return ''.join(decoded)\n",
        "\n",
        "def calculate_wer(reference, hypothesis):\n",
        "    ref_words = reference.split()\n",
        "    hyp_words = hypothesis.split()\n",
        "    if len(ref_words) == 0:\n",
        "        return 0.0 if len(hyp_words) == 0 else 1.0\n",
        "    distance = Levenshtein.distance(' '.join(ref_words), ' '.join(hyp_words))\n",
        "    return distance / len(ref_words)\n",
        "\n",
        "def calculate_cer(reference, hypothesis):\n",
        "    if len(reference) == 0:\n",
        "        return 0.0 if len(hypothesis) == 0 else 1.0\n",
        "    distance = Levenshtein.distance(reference, hypothesis)\n",
        "    return distance / len(reference)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#   E V A L U A T I O N    W I T H    G P U / M P S / C P U\n",
        "# ============================================================\n",
        "\n",
        "def evaluate_model(model, dataloader, model_type='MLP', device=None, num_samples=None):\n",
        "\n",
        "    # -----------------------------\n",
        "    # Device selection\n",
        "    # -----------------------------\n",
        "    if device is None:\n",
        "        device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available()\n",
        "            else \"mps\" if torch.backends.mps.is_available()\n",
        "            else \"cpu\"\n",
        "        )\n",
        "    print(f\"\\nEvaluating on device: {device}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    chars = \"abcdefghijklmnopqrstuvwxyz '\"\n",
        "    char2idx = {c: i for i, c in enumerate(chars)}\n",
        "    idx2char = {i: c for c, i in char2idx.items()}\n",
        "\n",
        "    total_wer = 0.0\n",
        "    total_cer = 0.0\n",
        "    num_processed = 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (waveforms, sample_rates, transcripts, _, _, _) in enumerate(dataloader):\n",
        "\n",
        "            for i, (waveform, sample_rate, transcript) in enumerate(\n",
        "                zip(waveforms, sample_rates, transcripts)\n",
        "            ):\n",
        "\n",
        "                waveform = waveform.to(device)\n",
        "\n",
        "                # ---------------------------------------------------\n",
        "                # Feature extraction (GPU where possible)\n",
        "                # ---------------------------------------------------\n",
        "                if model_type == 'MLP':\n",
        "\n",
        "                    features = _MFCC(waveform, sample_rate)  # your MFCC fn\n",
        "                    features = features.squeeze(0).transpose(0, 1)\n",
        "                    features = features.to(device)\n",
        "\n",
        "                elif model_type == 'CNN':\n",
        "\n",
        "                    mel_spec_transform = torchaudio.transforms.MelSpectrogram(\n",
        "                        sample_rate=sample_rate,\n",
        "                        n_mels=40,\n",
        "                        n_fft=400,\n",
        "                        hop_length=160\n",
        "                    )\n",
        "\n",
        "                    # Some torchaudio kernels cannot run on MPS/GPU → move after transform\n",
        "                    mel_spec_transform = mel_spec_transform.to(device)\n",
        "\n",
        "                    features = mel_spec_transform(waveform)\n",
        "                    features = features.clamp(min=1e-9).log2()\n",
        "                    features = features.unsqueeze(0).to(device)\n",
        "\n",
        "                elif model_type == 'GRU':\n",
        "\n",
        "                    mel_spec_transform = torchaudio.transforms.MelSpectrogram(\n",
        "                        sample_rate=sample_rate,\n",
        "                        n_mels=40,\n",
        "                        n_fft=400,\n",
        "                        hop_length=160\n",
        "                    )\n",
        "\n",
        "                    mel_spec_transform = mel_spec_transform.to(device)\n",
        "\n",
        "                    features = mel_spec_transform(waveform)\n",
        "                    features = features.clamp(min=1e-9).log2()\n",
        "                    # GRU expects [B, T, F]\n",
        "                    features = features.squeeze(0).transpose(0, 1).unsqueeze(0).to(device)  # [1, T, 40]\n",
        "\n",
        "                elif model_type == 'Transformer':\n",
        "\n",
        "                    mel_spec_transform = torchaudio.transforms.MelSpectrogram(\n",
        "                        sample_rate=sample_rate,\n",
        "                        n_mels=40,\n",
        "                        n_fft=400,\n",
        "                        hop_length=160\n",
        "                    )\n",
        "\n",
        "                    mel_spec_transform = mel_spec_transform.to(device)\n",
        "\n",
        "                    features = mel_spec_transform(waveform)\n",
        "                    features = features.clamp(min=1e-9).log2()\n",
        "                    # Transformer expects [B, T, F]\n",
        "                    features = features.squeeze(0).transpose(0, 1).unsqueeze(0).to(device)  # [1, T, 40]\n",
        "\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown model_type: {model_type}\")\n",
        "\n",
        "                # Forward → logits\n",
        "                logits = model(features)\n",
        "                if model_type == 'CNN':\n",
        "                    logits = logits.squeeze(0)\n",
        "                elif model_type == 'GRU':\n",
        "                    logits = logits.squeeze(0)\n",
        "                elif model_type == 'Transformer':\n",
        "                    logits = logits.squeeze(0)\n",
        "\n",
        "                log_probs = F.log_softmax(logits, dim=1)\n",
        "\n",
        "                # Decode text\n",
        "                predicted_text = decode_predictions(log_probs, idx2char)\n",
        "                reference_text = transcript.lower()\n",
        "\n",
        "                # Metrics\n",
        "                wer = calculate_wer(reference_text, predicted_text)\n",
        "                cer = calculate_cer(reference_text, predicted_text)\n",
        "\n",
        "                total_wer += wer\n",
        "                total_cer += cer\n",
        "                num_processed += 1\n",
        "\n",
        "                # Print first few samples\n",
        "                if num_processed <= 5:\n",
        "                    print(f\"\\nSample {num_processed}:\")\n",
        "                    print(f\"  Reference:  {reference_text}\")\n",
        "                    print(f\"  Predicted:  {predicted_text}\")\n",
        "                    print(f\"  WER: {wer:.2%}, CER: {cer:.2%}\")\n",
        "\n",
        "                if num_samples and num_processed >= num_samples:\n",
        "                    break\n",
        "\n",
        "            if num_samples and num_processed >= num_samples:\n",
        "                break\n",
        "\n",
        "    avg_wer = total_wer / num_processed if num_processed > 0 else 0\n",
        "    avg_cer = total_cer / num_processed if num_processed > 0 else 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"AVERAGE METRICS (over {num_processed} samples)\")\n",
        "    print(f\"  Average WER: {avg_wer:.2%}\")\n",
        "    print(f\"  Average CER: {avg_cer:.2%}\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    return avg_wer, avg_cer\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#     L O A D   &   E V A L U A T E\n",
        "# ============================================================\n",
        "\n",
        "def load_and_evaluate(model_path=None, model_type='MLP', batch_size=8, num_workers=2, num_samples=50):\n",
        "\n",
        "    # -----------------------------\n",
        "    # Device selection\n",
        "    # -----------------------------\n",
        "    device = torch.device(\n",
        "        \"cuda\" if torch.cuda.is_available()\n",
        "        else \"mps\" if torch.backends.mps.is_available()\n",
        "        else \"cpu\"\n",
        "    )\n",
        "    print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "    # Dataset\n",
        "    eval_dataset = LIBRISPEECH(\"./data\", url=\"dev-clean\", download=True)\n",
        "    eval_loader = DataLoader(\n",
        "        eval_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # Model init\n",
        "    print(f\"Initializing {model_type} model...\")\n",
        "    if model_type == 'MLP':\n",
        "        model = MLP(input_size=13, hidden_size=128, output_size=29)\n",
        "    elif model_type == 'CNN':\n",
        "        model = CNN(n_classes=29, n_mels=40)\n",
        "    elif model_type == 'GRU':\n",
        "        model = GRU(input_dim=40, hidden_dim=128, num_layers=2, n_classes=29)\n",
        "    elif model_type == 'Transformer':\n",
        "        model = Transformer(input_dim=40, d_model=256, nhead=8, num_layers=6, n_classes=29)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "    # Load weights\n",
        "    if model_path:\n",
        "        print(f\"Loading model from {model_path}\")\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    else:\n",
        "        print(\"Warning: No model_path provided → evaluating random weights\")\n",
        "\n",
        "    # Evaluate\n",
        "    return evaluate_model(\n",
        "        model,\n",
        "        eval_loader,\n",
        "        model_type=model_type,\n",
        "        device=device,\n",
        "        num_samples=num_samples\n",
        "    )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wM3pf-Is8oWw"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP\n",
        "train_model(batch_size=8, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmUJreLe83am",
        "outputId": "08d18894-a257-4b98-857c-28656b4f63ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Data Downloaded !\n",
            "Dataset size: 28539 samples\n",
            "First sample shape: torch.Size([1, 225360])\n",
            "Using Transformer\n",
            "Epoch 1, Batch 10, Avg Loss: 6.4464\n",
            "Epoch 1, Batch 20, Avg Loss: 4.6797\n",
            "Epoch 1, Batch 30, Avg Loss: 4.0824\n",
            "Epoch 1, Batch 40, Avg Loss: 3.7850\n",
            "Epoch 1, Batch 50, Avg Loss: 3.6060\n",
            "Epoch 1, Batch 60, Avg Loss: 3.4881\n",
            "Epoch 1, Batch 70, Avg Loss: 3.4064\n",
            "Epoch 1, Batch 80, Avg Loss: 3.3411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_and_evaluate(\n",
        "    model_path='gru.pth',\n",
        "    model_type='Transformers',\n",
        "    num_samples=20\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fL52LXl-IIti",
        "outputId": "ec3f4a22-af6d-4c35-eead-590fdcc23a7e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using device: cuda\n",
            "Initializing GRU model...\n",
            "Loading model from gru.pth\n",
            "\n",
            "Evaluating on device: cuda\n",
            "\n",
            "================================================================================\n",
            "EVALUATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "Sample 1:\n",
            "  Reference:  mister quilter is the apostle of the middle classes and we are glad to welcome his gospel\n",
            "  Predicted:  itecuters ipuo melcasis er  loe is guto\n",
            "  WER: 347.06%, CER: 65.17%\n",
            "\n",
            "Sample 2:\n",
            "  Reference:  nor is mister quilter's manner less interesting than his matter\n",
            "  Predicted:  norismeteoers mam lisenvrtin then is mater\n",
            "  WER: 280.00%, CER: 44.44%\n",
            "\n",
            "Sample 3:\n",
            "  Reference:  he tells us that at this festive season of the year with christmas and roast beef looming before us similes drawn from eating and its results occur most readily to the mind\n",
            "  Predicted:  etois the e thetueesonno er wi cemesrs bein befrs solas orntoeting orso e cr marl ote m\n",
            "  WER: 318.75%, CER: 59.30%\n",
            "\n",
            "Sample 4:\n",
            "  Reference:  he has grave doubts whether sir frederick leighton's work is really greek after all and can discover in it but little of rocky ithaca\n",
            "  Predicted:  hes gra dos mterserfre lis ir ira gre te iindeerini b mi o rti   cu\n",
            "  WER: 325.00%, CER: 58.65%\n",
            "\n",
            "Sample 5:\n",
            "  Reference:  linnell's pictures are a sort of up guards and at em paintings and mason's exquisite idylls are as national as a jingo poem mister birket foster's landscapes smile at one much in the same way that mister carker used to flash his teeth and mister john collier gives his sitter a cheerful slap on the back before he says like a shampooer in a turkish bath next man\n",
            "  Predicted:  winos eters rsir  bodtead andins anmisis wisitit ris atatiin o etebere futeristits  n mutinin witietru atefasistt anmitertorca isisitr turoslapnbat e furiis wa aa mapr o ni men\n",
            "  WER: 351.47%, CER: 65.47%\n",
            "\n",
            "================================================================================\n",
            "AVERAGE METRICS (over 20 samples)\n",
            "  Average WER: 325.93%\n",
            "  Average CER: 59.88%\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3.259282575603885, 0.5988340362371821)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uWlHi3yBIYKz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}